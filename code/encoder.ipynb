{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "encoder.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxpumCnMwVHk"
      },
      "source": [
        "IMPORTING COMPRESSION AND OTHER RELATED PACKAGES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bScSltWYL-C"
      },
      "source": [
        "\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "#from keras.applications.resnet50 import ResNet50\n",
        "#from keras.applications.resnet50 import preprocess_input\n",
        "from keras.models import Model\n",
        "from os import listdir\n",
        "import tqdm\n",
        "import pickle as pk\n",
        "import pathlib\n",
        "import string \n",
        "\n",
        "\n",
        "import tensorflow as tf \n",
        "import tensorflow_model_optimization as tfmot\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "pip install -q tensorflow-model-optimization\n",
        "\n",
        "\n",
        "\n",
        "from keras import callbacks\n",
        "from keras import optimizers\n",
        "from keras.engine import Model\n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D,BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import np_utils\n",
        "\n",
        "\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.random import rand\n",
        "\n",
        "pd.options.display.max_colwidth = 600\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdoXAkX_YR7F"
      },
      "source": [
        "folder_path = '###'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ0W-hWUwhdz"
      },
      "source": [
        "FEATURE EXTRACTION USING UNQUANTIZED MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUaVdSt9Yfq_"
      },
      "source": [
        "\n",
        "def extract_features(directory):\n",
        "\t# load the model\n",
        "\tmodel,features = VGG16(),dict()\n",
        " \n",
        "\tmodel = Model(inputs=model.input, outputs=model.layers[-4].output)  # Before all dense layers\n",
        " \n",
        "\t# extract features from each photo\n",
        "\n",
        "\tfor name in tqdm(listdir(directory)):\n",
        "\t\t# load an image from file\n",
        "\t\tfilename = directory + '/' + name\n",
        "\t\t\n",
        "\t\timage = load_img(filename, target_size=(224, 224))\n",
        "\t\t# convert the image pixels to a numpy array\n",
        "\t\timage = img_to_array(image)\n",
        "\t\t# reshape data for the model\n",
        "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t\t# prepare the image for the VGG model\n",
        "\t\timage = preprocess_input(image)\n",
        "\t\t# get features\n",
        "\t\tfeature = model.predict(image, verbose=0)\t\n",
        "\t\t# get image id\n",
        "\t\timage_id = name.split('.')[0]\n",
        "\t  # Store feature with its key in dictionary\n",
        "\t\tfeature = feature.reshape(1,-1)\n",
        "\t\tfeatures[image_id] = feature\n",
        "\treturn features\n",
        " \n",
        "# extract features from all images\n",
        "directory = folder_path + '/Flicker8k_Dataset'\n",
        "features = extract_features(directory)\n",
        "print('Extracted Features: %d' % len(features))\n",
        "# save to file\n",
        "pk.dump(features, open(folder_path + '#', 'wb'))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97WHHLCK-xEZ"
      },
      "source": [
        "TRAINING AND SUBSEQUENT PRUNING OF CNN USING CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0zqOJdb-400"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "NUM_CLASSES = 100\n",
        "LEARNING_RATE = 1e-4\n",
        "MOMENTUM = 0.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRrXvQ_v_F0q"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar100.load_data(label_mode='fine')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r7tD90T_Iw9"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
        "                                                  y_train, \n",
        "                                                  test_size=0.15, \n",
        "                                                  stratify=np.array(y_train), \n",
        "                                                  random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Ggg9LX_MaN"
      },
      "source": [
        "Y_train = np_utils.to_categorical(y_train, NUM_CLASSES)\n",
        "Y_val = np_utils.to_categorical(y_val, NUM_CLASSES)\n",
        "Y_test = np_utils.to_categorical(y_test, NUM_CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaIi__tJ_b5Y"
      },
      "source": [
        "X_train = np.array([np.array(Image.fromarray(x).resize((48, 48))) for x in X_train])\n",
        "X_val = np.array([np.array(Image.fromarray(x).resize((48, 48))) for x in X_val])\n",
        "X_test = np.array([np.array(Image.fromarray(x)).resize((48, 48)) for x in X_test])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW5raun8_fR1"
      },
      "source": [
        "base_model = ResNet50(weights='imagenet', \n",
        "                       include_top=False, \n",
        "                       input_shape=(48, 48, 3))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iib-3KM7xrJ7"
      },
      "source": [
        "\n",
        "pruning_params = {\n",
        "    'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.50, 0),\n",
        "    'block_size': (1, 1),\n",
        "    'block_pooling_type': 'AVG'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbnJX0jXxtO2"
      },
      "source": [
        "FOR PRUNING RESNET-50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_08UfQzYECjn"
      },
      "source": [
        "\n",
        "last = base_model.output\n",
        "x = GlobalAveragePooling2D()(last)\n",
        "x = Dropout(0.6)(x)\n",
        "x= BatchNormalization()(x)\n",
        "last_dense = tfmot.sparsity.keras.prune_low_magnitude(Dense(1000, activation='relu'),**pruning_params)(x)\n",
        "\n",
        "pred = Dense(NUM_CLASSES, activation='softmax')(last_dense)\n",
        "model = Model(base_model.input, pred)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibITVzGxx1L7"
      },
      "source": [
        "FOR PRUNING VGG-16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbNvBEi6xgO7"
      },
      "source": [
        "last = base_model.get_layer('block3_pool').output\n",
        "x = GlobalAveragePooling2D()(last)\n",
        "x= BatchNormalization()(x)\n",
        "x = tfmot.sparsity.keras.prune_low_magnitude(Dense(256, activation='relu'),**pruning_params)(x)\n",
        "\n",
        "x = tfmot.sparsity.keras.prune_low_magnitude(Dense(256, activation='relu'),**pruning_params)(x)\n",
        "last_dense = Dropout(0.6)(x)\n",
        "pred = Dense(NUM_CLASSES, activation='softmax')(last_dense)\n",
        "model = Model(base_model.input, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJuIwUKjFLGj"
      },
      "source": [
        "for layer in base_model.layers:\n",
        "     layer.trainable = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbBmpDeCFbs0"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.Adam(lr=LEARNING_RATE),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxh6BthiFdmO"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcs1dECdFpSO"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    horizontal_flip=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMq46QAGGHLp"
      },
      "source": [
        "train_datagen.fit(X_train)\n",
        "train_generator = train_datagen.flow(X_train,\n",
        "                                     Y_train, \n",
        "                                     batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF4ZJK4eGJYA"
      },
      "source": [
        "val_datagen = ImageDataGenerator(rescale=1. / 255,\n",
        "    horizontal_flip=False)\n",
        "\n",
        "val_datagen.fit(X_val)\n",
        "val_generator = val_datagen.flow(X_val,\n",
        "                                 Y_val,\n",
        "                                 batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LioVh8mrGMBK"
      },
      "source": [
        "train_steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
        "val_steps_per_epoch = X_val.shape[0] // BATCH_SIZE\n",
        "callback = [tfmot.sparsity.keras.UpdatePruningStep()]\n",
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch=train_steps_per_epoch,\n",
        "                              validation_data=val_generator,\n",
        "                              validation_steps=val_steps_per_epoch,\n",
        "                              epochs= EPOCHS,\n",
        "                              verbose = 1,\n",
        "                              callbacks = callback)\n",
        "pruned_model = tfmot.sparsity.keras.strip_pruning(model)\n",
        "\n",
        "pruned_model.save(folder_path + '#' + '.h5')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de8YE-KgyGfH"
      },
      "source": [
        "FEATURE EXTRACTION USING QUANTIZED MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNOY6caDlnim"
      },
      "source": [
        "def getQuantizedModel(model):\n",
        "\n",
        "  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "  \n",
        "  tflite_model = converter.convert()\n",
        "  \n",
        "  tflite_models_dir = pathlib.Path(\"#\")\n",
        "  tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "  tflite_model_file = tflite_models_dir/\"no_Q_model.tflite\"\n",
        "  tflite_model_file.write_bytes(tflite_model)\n",
        "\n",
        "  \n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  tflite_quant_model = converter.convert()\n",
        "  tflite_model_quant_file = tflite_models_dir/\"Q_model.tflite\"\n",
        "  tflite_model_quant_file.write_bytes(tflite_quant_model)\n",
        "\n",
        "  interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
        "  interpreter_quant.allocate_tensors()\n",
        "\n",
        " \n",
        "  return interpreter_quant"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxUvijK_GRqn"
      },
      "source": [
        "# extract features from each photo in the directory\n",
        "def extract_features(directory,pruned_model,unpruned_model):\n",
        "\t# load the model\n",
        "\tmodel,features,base_model = pruned_model,dict(), unpruned_model\n",
        "  \n",
        "\tmodel, pruned_model = Model(inputs= base_model.inputs, outputs = last_dense), tfmot.sparsity.keras.strip_pruning(model)\n",
        "  \n",
        "\tquantized_model = getQuantizedModel(model)\n",
        " \n",
        "\tinput_index = quantized_model.get_input_details()[0][\"index\"]\n",
        "\t\n",
        "\toutput_index = quantized_model.get_output_details()[0][\"index\"]\n",
        " \n",
        "\t# extract features from each photo\n",
        "\n",
        "\tfor name in tqdm(listdir(directory)):\n",
        "\t\t# load an image from file\n",
        "\t\tfilename = directory + '/' + name\n",
        "\t\t\n",
        "\t\timage = load_img(filename, target_size=(48, 48))\n",
        "    \n",
        "\t\t# convert the image pixels to a numpy array\n",
        "\t\timage = img_to_array(image)\n",
        "\n",
        "    \n",
        "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t\n",
        "\t\t# prepare the image for the VGG model \n",
        "\t\timage = preprocess_input(image)\n",
        "\t\t# get features\n",
        "\n",
        "\t\tquantized_model.set_tensor(input_index, image)\n",
        "\t\n",
        "\t\t# Run inference.\n",
        "\t\tfeature = quantized_model.invoke()\n",
        "\t\n",
        "\t\t# get image id\n",
        "\t\timage_id = name.split('.')[0]\n",
        "\t\tfeature = quantized_model.get_tensor(output_index)[0]\n",
        "    \n",
        "\t  # Store feature with its key in dictionary\n",
        "\t\tfeature = feature.reshape(1,-1)\n",
        "\t\n",
        "\t\tfeatures[image_id] = feature\n",
        "\t\t\n",
        "\n",
        "\treturn features\n",
        " \n",
        "# extract features from all images\n",
        "directory = folder_path + '/Flicker8k_Dataset'\n",
        "\n",
        "print(pruned_model.summary())\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJCd7ujXlAFA"
      },
      "source": [
        "features = extract_features(directory,model,base_model)\n",
        "print('Extracted Features: %d' % len(features))\n",
        "# save to file\n",
        "pk.dump(features, open(folder_path + '#', 'wb'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_IkQRMolShA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}